{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Dropout,BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightInit(shape,dtype=None):\n",
    "    \"\"\"Initialize weights\"\"\"\n",
    "    return rng.normal(loc = 0.0, scale = 1e-2, size = shape)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def biasInit(shape,dtype=None):\n",
    "    \"\"\"Initialize bias\"\"\"\n",
    "    return rng.normal(loc = 0.5, scale = 1e-2, size = shape)\n",
    "input_shape = (300, 300, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(6,6),activation='relu',input_shape=input_shape,kernel_initializer=weightInit,kernel_regularizer=l2(2e-4)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(128,(4,4),activation='relu',kernel_regularizer=l2(2e-4),kernel_initializer=weightInit,bias_initializer=biasInit))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer=weightInit,kernel_regularizer=l2(2e-4),bias_initializer=biasInit))\n",
    "# model.add(MaxPooling2D())\n",
    "# model.add(Conv2D(256,(2,2),activation='relu',kernel_initializer=weightInit,kernel_regularizer=l2(2e-4),bias_initializer=biasInit))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=weightInit,bias_initializer=biasInit))\n",
    "\n",
    "#call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "#layer to merge two encoded inputs with the l1 distance between them\n",
    "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#call this layer on list of two input tensors.\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=biasInit)(L1_distance)\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "optimizer = Adam(0.00006)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DatasetPrepare.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createLabelsTargets = CreateLabelsTargets(\"train_HB.csv\")\n",
    "createLabelsTargets.load_csv_and_treat() \n",
    "(X_train, X_test, y_train, y_test) = createLabelsTargets.create_labels_and_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesPath= os.getcwd()+'/modelHB_imgs/train/'\n",
    "# Creating training data generator\n",
    "dataTrainLoader = DataLoader(imagesPath,X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training loop\n",
    "evaluate_every = 50 # interval for evaluating on one-shot tasks\n",
    "loss_every= 50 # interval for printing loss (iterations)\n",
    "batch_size = 16\n",
    "epochs = 9000\n",
    "N_way = 5 # how many classes for testing one-shot tasks\n",
    "n_val = 100 #number of one-shot tasks to validate on\n",
    "lossHistory = []\n",
    "val_accHistory = []\n",
    "print(\"training\")\n",
    "for i in range(1, epochs):\n",
    "    (inputs,targets)=dataTrainLoader.getBatch(batch_size)\n",
    "    (loss)=siamese_net.train_on_batch(inputs,targets)\n",
    "    lossHistory.append(loss)\n",
    "    if i % evaluate_every == 0:\n",
    "        val_acc = dataTrainLoader.test_oneshot(siamese_net,N_way,n_val,X_test,y_test,verbose=True)\n",
    "        val_accHistory.append(val_acc)\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, loss: {:.2f}, val_acc: {:.2f}\".format(i,loss,val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossHistory)\n",
    "plt.show()\n",
    "plt.plot(val_accHistory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usar evaluate para ver loss\n",
    "# passar no teste x_train e X_test, para ver se Ã© overfitting\n",
    "#suspeita de overfitting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
