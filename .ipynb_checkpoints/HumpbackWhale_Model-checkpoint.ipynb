{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Dropout,BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightInit(shape,dtype=None):\n",
    "    \"\"\"Initialize weights\"\"\"\n",
    "    return rng.normal(loc = 0.0, scale = 1e-2, size = shape)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def biasInit(shape,dtype=None):\n",
    "    \"\"\"Initialize bias\"\"\"\n",
    "    return rng.normal(loc = 0.5, scale = 1e-2, size = shape)\n",
    "input_shape = (300, 300, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(6,6),activation='relu',input_shape=input_shape,kernel_initializer=weightInit,kernel_regularizer=l2(2e-4)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(128,(4,4),activation='relu',kernel_regularizer=l2(2e-4),kernel_initializer=weightInit,bias_initializer=biasInit))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer=weightInit,kernel_regularizer=l2(2e-4),bias_initializer=biasInit))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(256,(2,2),activation='relu',kernel_initializer=weightInit,kernel_regularizer=l2(2e-4),bias_initializer=biasInit))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=weightInit,bias_initializer=biasInit))\n",
    "\n",
    "#call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "#layer to merge two encoded inputs with the l1 distance between them\n",
    "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#call this layer on list of two input tensors.\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=biasInit)(L1_distance)\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "optimizer = Adam(0.00006)\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300, 300, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 300, 300, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 4096)         1212571968  input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 4096)         0           sequential[0][0]                 \n",
      "                                                                 sequential[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            4097        lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,212,576,065\n",
      "Trainable params: 1,212,575,297\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DatasetPrepare.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "createLabelsTargets = CreateLabelsTargets(\"train_HB.csv\")\n",
    "createLabelsTargets.load_csv_and_treat() \n",
    "(X_train, X_test, y_train, y_test) = createLabelsTargets.create_labels_and_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesPath= os.getcwd()+'/modelHB_imgs/train/'\n",
    "# Creating training data generator\n",
    "dataTrainLoader = DataLoader(imagesPath,X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "Evaluating model on 100 random 5 way one-shot learning tasks ...\n",
      "Got an average of 29.0% 5 way one-shot learning accuracy\n",
      "iteration 50, loss: 114.15, val_acc: 29.00\n",
      "Evaluating model on 100 random 5 way one-shot learning tasks ...\n",
      "Got an average of 29.0% 5 way one-shot learning accuracy\n",
      "iteration 100, loss: 107.50, val_acc: 29.00\n",
      "Evaluating model on 100 random 5 way one-shot learning tasks ...\n",
      "Got an average of 11.0% 5 way one-shot learning accuracy\n",
      "iteration 150, loss: 100.62, val_acc: 11.00\n",
      "Evaluating model on 100 random 5 way one-shot learning tasks ...\n",
      "Got an average of 33.0% 5 way one-shot learning accuracy\n",
      "iteration 200, loss: 94.81, val_acc: 33.00\n",
      "Evaluating model on 100 random 5 way one-shot learning tasks ...\n",
      "Got an average of 25.0% 5 way one-shot learning accuracy\n",
      "iteration 250, loss: 89.14, val_acc: 25.00\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "evaluate_every = 50 # interval for evaluating on one-shot tasks\n",
    "loss_every= 50 # interval for printing loss (iterations)\n",
    "checkOverfitting = 100\n",
    "batch_size = 16\n",
    "epochs = 9000\n",
    "N_way = 5 # how many classes for testing one-shot tasks\n",
    "n_val = 100 #number of one-shot tasks to validate on\n",
    "lossHistory = []\n",
    "val_accHistory = []\n",
    "print(\"training\")\n",
    "for i in range(1, epochs):\n",
    "    (inputs,targets)=dataTrainLoader.getBatch(batch_size)\n",
    "    (loss)=siamese_net.train_on_batch(inputs,targets)\n",
    "    lossHistory.append(loss)\n",
    "    if i % evaluate_every == 0:\n",
    "        val_acc = dataTrainLoader.test_oneshot(siamese_net,N_way,n_val,X_test,y_test,verbose=True)\n",
    "        val_accHistory.append(val_acc)\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, loss: {:.2f}, val_acc: {:.2f}\".format(i,loss,val_acc))\n",
    "    if i % checkOverfitting == 0:\n",
    "        print(\"Comparing results with x_train and x_test to check overfitting\")\n",
    "        val_acc_test = dataTrainLoader.test_oneshot(siamese_net,N_way,n_val,X_test,y_test,verbose=True)\n",
    "        val_acc_train = dataTrainLoader.test_oneshot(siamese_net,N_way,n_val,X_train,y_train,verbose=True)\n",
    "        print(\"Accuracy in train {:.2f}, accuracy in test {:.2f} \".format(val_acc_train,val_acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossHistory)\n",
    "plt.show()\n",
    "plt.plot(val_accHistory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usar evaluate para ver loss\n",
    "# passar no teste x_train e X_test, para ver se Ã© overfitting. Fazer uma funcao para comparar os resultados dos dois\n",
    "#suspeita de overfitting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
