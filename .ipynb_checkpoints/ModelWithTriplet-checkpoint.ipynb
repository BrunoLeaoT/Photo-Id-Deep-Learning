{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Dropout,BatchNormalization,Layer,GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        anchor, positive, negative = inputs\n",
    "        positiveDist = K.sum(K.square(anchor-positive), axis=-1)\n",
    "        negativeDist = K.sum(K.square(anchor-negative), axis=-1)\n",
    "        return K.sum(K.maximum(positiveDist - negativeDist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_shape, embeddingsize):\n",
    "    '''\n",
    "    Define the neural network to learn image similarity\n",
    "    Input : \n",
    "            input_shape : shape of input images\n",
    "            embeddingsize : vectorsize used to encode our picture   \n",
    "    '''\n",
    "     # Convolutional Neural Network\n",
    "    network = Sequential()\n",
    "    network.add(Conv2D(128, (7,7), activation='relu',input_shape=input_shape,kernel_initializer='he_uniform',kernel_regularizer=l2(2e-4)))\n",
    "    network.add(MaxPooling2D())\n",
    "    network.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform',kernel_regularizer=l2(2e-4)))\n",
    "    network.add(MaxPooling2D())\n",
    "    network.add(Conv2D(256, (3,3), activation='relu', kernel_initializer='he_uniform',kernel_regularizer=l2(2e-4)))\n",
    "    network.add(GlobalAveragePooling2D())\n",
    "    network.add(Dense(4096, activation='relu',kernel_regularizer=l2(1e-3),kernel_initializer='he_uniform'))\n",
    "    \n",
    "    \n",
    "    network.add(Dense(embeddingsize, activation=None,kernel_regularizer=l2(1e-3),kernel_initializer='he_uniform'))\n",
    "    \n",
    "    #Force the encoding to live on the d-dimentional hypershpere\n",
    "    network.add(Lambda(lambda x: K.l2_normalize(x,axis=-1)))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputShape, network, margin=0.2):\n",
    "    '''\n",
    "    Define the Keras Model for training \n",
    "        Input : \n",
    "            input_shape : shape of input images\n",
    "            network : Neural network to train outputing embeddings\n",
    "            margin : minimal distance between Anchor-Positive and Anchor-Negative for the lossfunction (alpha)\n",
    "    \n",
    "    '''\n",
    "     # Define the tensors for the three input images\n",
    "    anchorInput = Input(inputShape, name=\"anchorInput\")\n",
    "    positiveInput = Input(inputShape, name=\"positiveInput\")\n",
    "    negativeInput = Input(inputShape, name=\"negativeInput\") \n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the three images\n",
    "    encodedA = network(anchorInput)\n",
    "    encodedP = network(positiveInput)\n",
    "    encodedN = network(negativeInput)\n",
    "    \n",
    "    #TripletLoss Layer\n",
    "    lossLayer = TripletLossLayer(alpha=margin,name='tripletLossLayer')([encodedA,encodedP,encodedN])\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    model = Model(inputs=[anchorInput,positiveInput,negativeInput],outputs=lossLayer)\n",
    "    optimizer = Adam(lr = 0.00006)\n",
    "    model.compile(loss=None,optimizer=optimizer)\n",
    "    # return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(a,b):\n",
    "    return np.sum(np.square(a-b))\n",
    "\n",
    "def compute_probs(network,X,Y):\n",
    "    '''\n",
    "    Input\n",
    "        network : current NN to compute embeddings\n",
    "        X : tensor of shape (m,w,h,1) containing pics to evaluate\n",
    "        Y : tensor of shape (m,) containing true class\n",
    "        \n",
    "    Returns\n",
    "        probs : array of shape (m,m) containing distances\n",
    "    \n",
    "    '''\n",
    "    # m number of images in X\n",
    "    m = len(X)\n",
    "    nbevaluation = int(m*(m-1)/2)\n",
    "    probs = np.zeros((nbevaluation))\n",
    "    y = np.zeros((nbevaluation))\n",
    "    \n",
    "    #Compute all embeddings for all pics with current network\n",
    "    embeddings = network.predict(X)\n",
    "    \n",
    "    size_embedding = embeddings.shape[1]\n",
    "    \n",
    "    #For each pics of our dataset\n",
    "    k = 0\n",
    "    for i in range(m):\n",
    "            #Against all other images\n",
    "            for j in range(i+1,m):\n",
    "                #compute the probability of being the right decision : it should be 1 for right class, 0 for all other classes\n",
    "                probs[k] = -compute_dist(embeddings[i,:],embeddings[j,:])\n",
    "                if (Y[i]==Y[j]):\n",
    "                    y[k] = 1\n",
    "                else:\n",
    "                    y[k] = 0\n",
    "                k += 1\n",
    "    return probs,y\n",
    "#probs,yprobs = compute_probs(network,x_test_origin[:10,:,:,:],y_test_origin[:10])\n",
    "def computeAccuracy(probs,y):\n",
    "    correctPrediction = 0\n",
    "    totalPredictions = len(probs)\n",
    "    probThreshholded = []\n",
    "    threshhold = 0.5\n",
    "    \n",
    "    # Creating vector of predicitions after passing through threshold\n",
    "    for prob in probs:\n",
    "        if(prob > 0.5):\n",
    "            probThreshholded.append(1)\n",
    "        else:\n",
    "            probThreshholded.append(0)\n",
    "            \n",
    "    for index,prob in enumerate(probThreshholded):\n",
    "        if(prob == y[index]): # If prediction equal to true target, add 1 to numer of corrects predictions\n",
    "            correctPrediction += 1\n",
    "    return correctPrediction/totalPredictions # Percentage of correct guesses\n",
    "def compute_metrics(probs,yprobs):\n",
    "    '''\n",
    "    Returns\n",
    "        fpr : Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i]\n",
    "        tpr : Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].\n",
    "        thresholds : Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1\n",
    "        auc : Area Under the ROC Curve metric\n",
    "    '''\n",
    "    # calculate AUC\n",
    "    auc = roc_auc_score(yprobs, probs)\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(yprobs, probs)\n",
    "    \n",
    "    return fpr, tpr, thresholds,au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DatasetPrepare.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsPath = os.getcwd()+'/modelHB_imgs/train/'\n",
    "csvPath = 'train_HB.csv'\n",
    "createLabelsTargets = CreateLabelsTargets(csvPath)\n",
    "createLabelsTargets.load_csv_and_treat()\n",
    "(X_train, X_test_original, y_train, y_test_original) = createLabelsTargets.create_dataset_with_image(imgsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchorInput (InputLayer)        [(None, 200, 200, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positiveInput (InputLayer)      [(None, 200, 200, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negativeInput (InputLayer)      [(None, 200, 200, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 64)           1764032     anchorInput[0][0]                \n",
      "                                                                 positiveInput[0][0]              \n",
      "                                                                 negativeInput[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tripletLossLayer (TripletLossLa ()                   0           sequential_1[0][0]               \n",
      "                                                                 sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,764,032\n",
      "Trainable params: 1,764,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputShape = (200,200,1)\n",
    "embeddingsize = 64\n",
    "network = build_network(inputShape, embeddingsize)\n",
    "model = build_model(inputShape, network, margin=0.2)\n",
    "dataLoader =DataLoader('',X_train,y_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateEvery = 10 # interval for evaluating on one-shot tasks\n",
    "batchSize = 32\n",
    "epochs = 400 # No. of training iterations\n",
    "nVal = 250 # how many one-shot tasks to validate on\n",
    "nIteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rehasping X_test to work on compute probs funtction\n",
    "X_test = []\n",
    "y_test = []\n",
    "for index,values in enumerate(X_test_original):\n",
    "    for value in values:\n",
    "        X_test.append(value)\n",
    "        y_test.append(y_test_original[index])\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Evaluating\n",
      "\n",
      " ------------- \n",
      "\n",
      "[10] Time for 10 iterations: 0.9 mins, Train Loss: 12.453065872192383\n",
      "Accuracy 0.9802088353413655\n",
      "Evaluating\n",
      "\n",
      " ------------- \n",
      "\n",
      "[20] Time for 20 iterations: 1.7 mins, Train Loss: 12.000187873840332\n",
      "Accuracy 0.9804658634538153\n",
      "Evaluating\n",
      "\n",
      " ------------- \n",
      "\n",
      "[30] Time for 30 iterations: 2.6 mins, Train Loss: 11.873855590820312\n",
      "Accuracy 0.968289156626506\n",
      "Evaluating\n",
      "\n",
      " ------------- \n",
      "\n",
      "[40] Time for 40 iterations: 3.5 mins, Train Loss: 11.923223495483398\n",
      "Accuracy 0.9768995983935743\n",
      "Evaluating\n",
      "\n",
      " ------------- \n",
      "\n",
      "[50] Time for 50 iterations: 4.3 mins, Train Loss: 12.640979766845703\n",
      "Accuracy 0.9553734939759037\n",
      "Evaluating\n",
      "\n",
      " ------------- \n",
      "\n",
      "[60] Time for 60 iterations: 5.2 mins, Train Loss: 12.80224609375\n",
      "Accuracy 0.9768674698795181\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "t_start = time.time()\n",
    "\n",
    "for i in range(1, epochs):\n",
    "    triplets = dataLoader.getBatchRandom(batchSize,X_train)\n",
    "    loss = model.train_on_batch(triplets, None)\n",
    "    nIteration += 1\n",
    "    if i % evaluateEvery == 0:\n",
    "        print(\"Evaluating\")\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        randomIndex = rng.randint(len(y_test))\n",
    "        if(randomIndex < nVal):\n",
    "            randomIndex += (nVal - randomIndex)\n",
    "        probs,yprob = compute_probs(network,X_test[(randomIndex-nVal):randomIndex],y_test[(randomIndex-nVal):randomIndex])\n",
    "        acc = computeAccuracy(probs,yprob)\n",
    "        print(\"[{3}] Time for {0} iterations: {1:.1f} mins, Train Loss: {2}\".format(i, (time.time()-t_start)/60.0,loss,nIteration))\n",
    "        print(\"Validating in test set[{},{}] Accuracy: {}\".format(randomIndex-nVal, randomIndex, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
